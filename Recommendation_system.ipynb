{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie Recommendation System based on Item based Collaborative Filtering\n",
    "By:\n",
    "\n",
    "Vaishnavi Gupta (N019)\n",
    "Khushi Vora (N078)\n",
    "Shubham Nagar (C023)\n",
    "Navin Sada (C054)\n",
    "A Recommendation System is a filtration program whose prime goal is to predict the “rating” or “preference” of a user towards a domain-specific item or item.\n",
    "\n",
    "There are mainly three types of recommendation systems:\n",
    "\n",
    "1) Content based recommendation system: The algorithm recommends products that are similar to the ones that a user has liked in the past. This similarity (generally cosine similarity) is computed from the data we have about the items as well as the user’s past preferences. For example, if a user likes movies such as ‘The Prestige’ then we can recommend him the movies of ‘Christian Bale’ or movies with the genre ‘Thriller’ or maybe even movies directed by ‘Christopher Nolan’.So what happens here the recommendation system checks the past preferences of the user and find the film “The Prestige”, then tries to find similar movies to that using the information available in the database such as the lead actors, the director, genre of the film, production house, etc and based on this information find movies similar to “The Prestige”.\n",
    "\n",
    "2) Collaborative recommendation system: This filtration strategy is based on the combination of the user’s behavior and comparing and contrasting that with other users’ behavior in the database. The history of all users plays an important role in this algorithm. The main difference between content-based filtering and collaborative filtering that in the latter, the interaction of all users with the items influences the recommendation algorithm while for content-based filtering only the concerned user’s data is taken into account. There are multiple ways to implement collaborative filtering but the main concept to be grasped is that in collaborative filtering multiple user’s data influences the outcome of the recommendation. and doesn’t depend on only one user’s data for modeling.\n",
    "\n",
    "Collaborative filtering is further classified into 2 categories:\n",
    "\n",
    "a) User based Collaborative filtering: The basic idea here is to find users that have similar past preference patterns as the user ‘A’ has had and then recommending him or her items liked by those similar users which ‘A’ has not encountered yet. This is achieved by making a matrix of items each user has rated/viewed/liked/clicked depending upon the task at hand, and then computing the similarity score between the users and finally recommending items that the concerned user isn’t aware of but users similar to him/her are and liked it.\n",
    "\n",
    "For example, if the user ‘A’ likes ‘Batman Begins’, ‘Justice League’ and ‘The Avengers’ while the user ‘B’ likes ‘Batman Begins’, ‘Justice League’ and ‘Thor’ then they have similar interests because we know that these movies belong to the super-hero genre. So, there is a high probability that the user ‘A’ would like ‘Thor’ and the user ‘B’ would like The Avengers’.\n",
    "\n",
    "b) Item based collaborative filtering: The concept in this case is to find similar movies instead of similar users and then recommending similar movies to that ‘A’ has had in his/her past preferences. This is executed by finding every pair of items that were rated/viewed/liked/clicked by the same user, then measuring the similarity of those rated/viewed/liked/clicked across all user who rated/viewed/liked/clicked both, and finally recommending them based on similarity scores.\n",
    "\n",
    "Here, for example, we take 2 movies ‘A’ and ‘B’ and check their ratings by all users who have rated both the movies and based on the similarity of these ratings, and based on this rating similarity by users who have rated both we find similar movies. So if most common users have rated ‘A’ and ‘B’ both similarly and it is highly probable that ‘A’ and ‘B’ are similar, therefore if someone has watched and liked ‘A’ they should be recommended ‘B’ and vice versa.\n",
    "\n",
    "In our case we have used Item based collaborative filtering\n",
    "\n",
    "Dataset used: The link for the dataset used from Kaggle is: https://www.kaggle.com/code/shivamb/netflix-shows-and-movies-exploratory-analysis/data\n",
    "\n",
    "The code used for the project is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "import sklearn.metrics\n",
    "import statsmodels.api as sm\n",
    "import plotly.express as px #for plotting the scatter plot\n",
    "import seaborn as sns #For plotting the dataset in seaborn\n",
    "sns.set(style='whitegrid')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the excel file\n",
    "data=pd.read_csv('/content/netflix_titles_nov_2019 new.csv')\n",
    "print(data.head(5))\n",
    "print(data.describe())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the columns of cast, date_added and listed_in as they are irrelevant. \n",
    "#Although the cast may be a factor depending on which a user might watch a movie, but the number of names in each entry, on label encoding, would lead to a lot of variables.\n",
    "#Thus we decided to drop it.\n",
    "data=data.drop(['cast','date_added','listed_in'],axis=1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a separate dataset named movie_title_desc which contains the show_id, title and description of the movie\n",
    "movie_title_desc=pd.DataFrame()\n",
    "movie_title_desc['title']=data['title']\n",
    "movie_title_desc['description']=data['description']\n",
    "movie_title_desc['show_id']=data['show_id']\n",
    "movie_title_desc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the columns: title and description from the main dataset as they have been added to the movie_title_desc dataframe\n",
    "data=data.drop(['title','description'],axis=1)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values in the dataset:\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the rows which have entries with null values of director and country\n",
    "data = data.dropna(axis = 0, how ='any',subset=['director','country'])\n",
    "# each entry in the duration column had entries like '126 min'.\n",
    "#Therefore, we had to remove the string 'min' from each entry\n",
    "data['duration'] = data['duration'].replace(\" min\",'',regex=True)\n",
    "\n",
    "print(data['duration'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding the values in director, rating, type and country as they have categorical data in them using the in-built function Label Encoder.\n",
    "le = LabelEncoder()\n",
    "data['director']= le.fit_transform(data['director'])\n",
    "data['rating']= le.fit_transform(data['rating'])\n",
    "data['type']= le.fit_transform(data['type'])\n",
    "data['country']= le.fit_transform(data['country']) \n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since ratings is a categorical feature data imputation is performed by replacing the null or nan values with the mode\n",
    "data['rating'] = data['rating'].fillna(data['rating'].mode()[0])\n",
    "\n",
    "# Converting each value in the duration column to float\n",
    "data['duration'].apply(lambda x: float(x))\n",
    "\n",
    "#replacing null or nan values with mean\n",
    "data['duration']=data['duration'].fillna(data['duration'].mean())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['show_id','duration','director','country','release_year','rating','type']\n",
    "for feature in features:\n",
    "    data[feature] = pd.to_numeric(data[feature], errors='coerce')\n",
    "# plotting the distribution plot for show_id, release_year and duration\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,3,1)\n",
    "sns.distplot(data['show_id'])\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.distplot(data['release_year'])\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.distplot(data['duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the boundary values for each column as we are using the z score treatment for outlier removal\n",
    "for column in data[features]:\n",
    "  print(\"Highest allowed in {} is:{}\".format(column,data[column].mean() + 3*data[column].std()))\n",
    "  print(\"Lowest allowed in {} is:{}\\n\\n\".format(column,data[column].mean() - 3*data[column].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the entries in the dataset outliers:\n",
    "data[(data['type'] > 0.0) | (data['type'] < 0.0)]\n",
    "data[(data['rating'] > 12.907489545791607) | (data['rating'] < 0.011238369402738257)]\n",
    "data[(data['release_year'] > 2041.0345281433606) | (data['release_year'] < 1984.2294022725132)]\n",
    "data[(data['country'] > 609.7975659892998) | (data['country'] < -91.93347248563032)]\n",
    "data[(data['director'] > 4007.76910860292) | (data['director'] < -1039.0183611171904)]\n",
    "data[(data['show_id'] > 110702147.1412701) | (data['show_id'] < 41767871.9361966)]\n",
    "data[(data['duration'] > 178.98291844704266) | (data['duration'] < 20.515314768505064)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trimming the outliers: cretaing a new dataframe new_df which contains only the outliers and then removing these entries from the main dataset\n",
    "\n",
    "new_df=data[(data['type'] > 0.5399688697419108) | (data['type'] < -0.48036971362376735)]\n",
    "new_df=data[(data['rating'] > 12.877465075058616) | (data['rating'] < 0.10038302620720518)]\n",
    "new_df=data[(data['release_year'] > 2041.000950704042) | (data['release_year'] < 2041.000950704042)]\n",
    "new_df=data[(data['country'] > 615.6713893723759) | (data['country'] < -92.0105243934729)]\n",
    "new_df=data[(data['director'] > 4138.199594880471) | (data['director'] < -1075.8383079606388)]\n",
    "new_df=data[(data['show_id'] > 110334417.6459286) | (data['show_id'] < 42278005.532341436)]\n",
    "new_df=data[(data['duration'] > 178.98291844704266) | (data['duration'] < 20.515314768505064)]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capping the outliers:\n",
    "upper_limit={}\n",
    "lower_limit={}\n",
    "for column in data[features]:\n",
    "  upper_limit[column]=data[column].mean() + 3*data[column].std()\n",
    "  lower_limit[column]=data[column].mean() - 3*data[column].std()\n",
    "print(upper_limit)\n",
    "print(lower_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the capping:\n",
    "for column in data[features]:\n",
    "  data[column] = np.where(\n",
    "    data[column]>upper_limit[column],\n",
    "    upper_limit[column],\n",
    "    np.where(\n",
    "        data[column]<lower_limit[column],\n",
    "        lower_limit[column],\n",
    "        data[column]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of correlation matrix with annotations in 2 different shades\n",
    "columns=['show_id','duration','director','country','release_year','rating','type']\n",
    "cor=data[columns].corr()\n",
    "hm1 = sns.heatmap(cor, annot = True)\n",
    "hm1.set(xlabel='\\nFeatures', ylabel='Features\\t', title = \"Correlation matrix of data\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing sparsity from the dataset by using csr_matrix function from the scipy library\n",
    "csr_data = csr_matrix(data.values)\n",
    "data.reset_index(inplace=True)\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using KNN algorithm to compute the distance between each movie in the dataset.\n",
    "#The input to the KNN model is the csr_matrix.\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)\n",
    "knn.fit(csr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for getting movie_recommendations.\n",
    "\n",
    "def get_movie_recommendation(movie_name):\n",
    "    n_movies_to_recommend = 10 #number of movies that we have to recommend\n",
    "    movie_list = movie_title_desc[movie_title_desc['title'].str.contains(movie_name)]  \n",
    "    if len(movie_list):        \n",
    "        movie_idx= movie_list.iloc[0]['show_id']\n",
    "        movie_idx = data[data['show_id'] == movie_idx].index[0]\n",
    "        distances , indices = knn.kneighbors(csr_data[movie_idx],n_neighbors= n_movies_to_recommend+1)    \n",
    "        rec_movie_indices = sorted(list(zip(indices.squeeze().tolist(),distances.squeeze().tolist())),key=lambda x: x[1])[:0:-1]\n",
    "        recommend_frame = []\n",
    "        for val in rec_movie_indices:\n",
    "            movie_idx = data.iloc[val[0]]['show_id']\n",
    "            idx = movie_title_desc[movie_title_desc['show_id'] == movie_idx].index\n",
    "            recommend_frame.append({'Title':movie_title_desc.iloc[idx]['title'].values[0],'Distance':val[1]})\n",
    "        df = pd.DataFrame(recommend_frame,index=range(1, n_movies_to_recommend+1))\n",
    "        return df\n",
    "    else:\n",
    "        return \"No movies found. Please check your input\"\n",
    "name=input(\"Please enter the name of your favourite or a recently watched movie that you loved:\\n\")\n",
    "get_movie_recommendation(name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
